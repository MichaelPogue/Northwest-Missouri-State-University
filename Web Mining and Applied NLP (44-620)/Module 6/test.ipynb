{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Polarity Score for the Wistera-Bonsai artcile is: 0.205\n"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "import requests\n",
    "import matplotlib.mlab as mlab\n",
    "import pickle\n",
    "import spacy\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "# List declarative elements\n",
    "parser = 'html.parser'\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "# Aquire Data\n",
    "article_page = requests.get('https://www.growabonsaitree.com/species/wisteria-bonsai/')\n",
    "article_html = article_page.text\n",
    "\n",
    "with open('wisteria.pkl', 'wb') as f:\n",
    "    pickle.dump(article_page.text, f)\n",
    "\n",
    "soup = BeautifulSoup(article_html, parser)\n",
    "\n",
    "with open('wisteria.pkl', 'rb') as f:\n",
    "    article_html = pickle.load(f)\n",
    "\n",
    "article_element = soup.find('article')\n",
    "article_text = article_element.get_text()\n",
    "doc = nlp(article_text.lower())\n",
    "\n",
    "soup = BeautifulSoup(article_html, parser)\n",
    "article_element = soup.find('article')\n",
    "\n",
    "p_score = round(doc._.blob.polarity, 3)\n",
    "\n",
    "print(f'The Polarity Score for the Wistera-Bonsai artcile is: {p_score}')\n",
    "\n",
    "def trim_extra(token):\n",
    "    return not (token.is_space or token.is_punct or token.is_stop)\n",
    "\n",
    "def token_isolation(token):\n",
    "    return not (token.is_space or token.is_punct or token.is_stop)\n",
    "\n",
    "# Define token score by sentence\n",
    "def score_sentence_by_token(sentence, interesting_tokens):\n",
    "    # Word Count\n",
    "    count = 0\n",
    "    for token in sentence:\n",
    "        if not (token.is_space or token.is_punct):\n",
    "            count += 1\n",
    "    \n",
    "    # Word occurrence count\n",
    "    occurrence =  len([i for i in interesting_tokens if i in sentence ])\n",
    "    \n",
    "    # Calculation\n",
    "    result = occurrence / count\n",
    "    return result, count, occurrence\n",
    "\n",
    "# Define llema score by sentence\n",
    "def score_sentence_by_lemma(sentence, interesting_lemmas):\n",
    "    # Word Count\n",
    "    count = 0\n",
    "    for token in sentence:\n",
    "        if not (token.is_space or token.is_punct):\n",
    "            count += 1\n",
    "    \n",
    "    # Word occurrence count\n",
    "    lemcount = 0\n",
    "    for token in sentence:\n",
    "        if token.lemma_.lower() in interesting_lemmas:\n",
    "            lemcount += 1\n",
    "    \n",
    "    # Calculation\n",
    "    result = lemcount / count\n",
    "    return result, count, lemcount\n",
    "\n",
    "# Find Tokens\n",
    "interesting_token = [token for token in doc if trim_extra(token)]\n",
    "token_freq = Counter(interesting_token)\n",
    "interesting_lemmas = [token.lemma_.lower() for token in doc if trim_extra(token)]\n",
    "lemma_freq = Counter(interesting_lemmas)\n",
    "\n",
    "# Pick most common\n",
    "top_tok = set()\n",
    "for tok, freq in token_freq.most_common(5):\n",
    "    top_tok.add(tok)\n",
    "\n",
    "top_lem = set()\n",
    "for lem, freq in lemma_freq.most_common(5):\n",
    "    top_lem.add(lem)\n",
    "\n",
    "sentences = list(doc.sents)\n",
    "sentence_list = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    if score_sentence_by_token(sentence, interesting_token)[0] > 0.6:\n",
    "        sentence_list.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'spacy.tokens.span.Span' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m sentence_list:\n\u001b[1;32m----> 2\u001b[0m     f\u001b[39m.\u001b[39mwrite(item \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'spacy.tokens.span.Span' and 'str'"
     ]
    }
   ],
   "source": [
    "for item in sentence_list:\n",
    "    f.write(item + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute '_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m p_score \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(sentence_list\u001b[39m.\u001b[39;49m_\u001b[39m.\u001b[39mblob\u001b[39m.\u001b[39mpolarity, \u001b[39m3\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute '_'"
     ]
    }
   ],
   "source": [
    "# Aquire Data\n",
    "article_page = sentence_list\n",
    "article_html = article_page.text\n",
    "\n",
    "soup = BeautifulSoup(article_html, parser)\n",
    "\n",
    "article_element = soup.find('article')\n",
    "article_text = article_element.get_text()\n",
    "doc = nlp(article_text.lower())\n",
    "\n",
    "soup = BeautifulSoup(article_html, parser)\n",
    "article_element = soup.find('article')\n",
    "\n",
    "p_score = round(doc._.blob.polarity, 3)\n",
    "\n",
    "p_score = round(sentence_list._.blob.polarity, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "863b3a55134ab839f8749329ac3d58c92f67b3b74d9daad2228f98af9c593ba4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
