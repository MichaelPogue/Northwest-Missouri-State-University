{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 - Ensembles \n",
    "Here is what we will do:\n",
    "1. Create a data set with two input features and a 3 category target\n",
    "2. Train a decision tree on the data set for a baseline\n",
    "3. Train 3 ensemble models\n",
    "    - Serial Ada Boosted DT\n",
    "    - Decision Forest\n",
    "    - Ensemble of DT, SVM, and NN\n",
    "3. Get model performance on train and test sets\n",
    "4. Create appropriate graphs\n",
    "5. Do a 10 fold cross validation with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings for the generated dataset\n",
    "This data set will be spirals with noise\n",
    "arms - This is the number of categories we will have\n",
    "turns - How many times we go around the spiral\n",
    "width - How wide is the spiral (percentage of the radius)\n",
    "noise - How much noise we will add to the data.\n",
    "size - The number of points to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "### Settings for the generated dataset\n",
    "arms = 3 # 3\n",
    "turns = 1.5 # 1.5\n",
    "width = 0.3 # 0.3\n",
    "noise = .25 # .25\n",
    "size = 4000 # 4000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The spirals will be based on the equation r=theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for NN on train data\n",
      "  Default settings\n",
      "Confusion Matrix\n",
      "[[1067    0    0]\n",
      " [   4 1062    0]\n",
      " [   0    0 1066]]\n",
      "Accuracy is   0.9987496092528916\n",
      "Precision is  0.9987542792463447\n",
      "Recall is     0.9987496092528916\n",
      "F1 is         0.9987496037562626\n",
      "\n",
      "Results for NN on test data\n",
      "  Default settings\n",
      "Confusion Matrix\n",
      "[[215  31  20]\n",
      " [ 27 218  22]\n",
      " [ 20  29 218]]\n",
      "Accuracy is   0.81375\n",
      "Precision is  0.8144072177959335\n",
      "Recall is     0.81375\n",
      "F1 is         0.8139055294405152\n"
     ]
    }
   ],
   "source": [
    "def build_arm(n):\n",
    "    points_per_arm = int(size/arms)\n",
    "    #Get points on the curve\n",
    "    theta_values = np.random.uniform(0, np.pi*turns, points_per_arm)\n",
    "    r_values = theta_values\n",
    "    target_values = [n] * points_per_arm\n",
    "    \n",
    "    #Add make the arm wider\n",
    "    lower_fraction = (1 - width)*theta_values\n",
    "    upper_fraction = (1 + width)*theta_values\n",
    "    difference = upper_fraction - lower_fraction\n",
    "    location = np.random.uniform(0, 1, points_per_arm)\n",
    "    theta_values = theta_values + lower_fraction + difference*location\n",
    "    \n",
    "    #offset angle \n",
    "    offset = n*2*np.pi/arms\n",
    "    theta_values = theta_values + offset\n",
    "    \n",
    "    #draw the noise from a distribution centered on 0.0 with a standard deviation of noise\n",
    "    x_noise_values = np.random.normal(0, noise, points_per_arm)\n",
    "    y_noise_values = np.random.normal(0, noise, points_per_arm)\n",
    "    x_values = r_values * np.sin(theta_values) + x_noise_values\n",
    "    y_values = r_values * np.cos(theta_values) + y_noise_values\n",
    "    \n",
    "    \n",
    "    return x_values, y_values, target_values\n",
    "    \n",
    "\n",
    "def build_set():\n",
    "    data_x = np.array([])\n",
    "    data_y = np.array([])\n",
    "    data_t = np.array([])\n",
    "    for i in range(0,arms):\n",
    "        x, y, t = build_arm(i)\n",
    "        #print(data_x)\n",
    "        #print(x)\n",
    "        #print(y)\n",
    "        #print(t)\n",
    "        # add each arm to the data set\n",
    "        data_x=np.append(data_x, x)\n",
    "        data_y=np.append(data_y, y)\n",
    "        data_t=np.append(data_t, t)\n",
    "     # create a dictionary with each feature\n",
    "    d = {}\n",
    "    d[\"A\"] = data_x\n",
    "    d[\"B\"] = data_y\n",
    "    d[\"Class\"] = data_t\n",
    "\n",
    "    #print(d)\n",
    "\n",
    "    # Create the data frame from the dictionary\n",
    "    \n",
    "    dataframe = pd.DataFrame(data=d)\n",
    "    return dataframe\n",
    "    \n",
    "\n",
    "spiral = build_set()\n",
    "#shuffle before plotting so we don't always overwrite with the same color\n",
    "shuffled = spiral.sample(frac=1)\n",
    "\n",
    "### Stratified Test/Train Data Split\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=123) #n_split ori 1\n",
    "for train_indices, test_indices in splitter.split(spiral, spiral['Class']):\n",
    "    train_set = spiral.iloc[train_indices]\n",
    "    test_set = spiral.iloc[test_indices]\n",
    "\n",
    "### Train and evaluate Random Forest model\n",
    "X = train_set[['A','B' ]]\n",
    "y = train_set['Class']\n",
    "\n",
    "X_test = test_set[['A','B']]\n",
    "y_test = test_set['Class']\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=50)\n",
    "rf_model.fit(X,y)\n",
    "\n",
    "y_pred = rf_model.predict(X)\n",
    "print('Results for NN on train data')\n",
    "print('  Default settings')\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print('Accuracy is  ', accuracy_score(y, y_pred))\n",
    "print('Precision is ', precision_score(y, y_pred, average='weighted'))\n",
    "print('Recall is    ', recall_score(y,y_pred, average='weighted'))\n",
    "print('F1 is        ', f1_score(y, y_pred, average='weighted'))\n",
    "print()\n",
    "\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "print('Results for NN on test data')\n",
    "print('  Default settings')\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print('Accuracy is  ', accuracy_score(y_test, y_test_pred))\n",
    "print('Precision is ', precision_score(y_test, y_test_pred, average='weighted'))\n",
    "print('Recall is    ', recall_score(y_test,y_test_pred, average='weighted'))\n",
    "print('F1 is        ', f1_score(y_test, y_test_pred, average='weighted'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Adjusting Size:\n",
    "\n",
    "| Model | Training Features | Acc Train | F1 Train |Acc Test | F1 Test |\n",
    "|:---|:---|:---|:---|:---|:---|\n",
    "|RFM|size 4000|1.0|1.0|79.75|79.74\n",
    "|RFM|size 8000|1.0|1.0|81.94|81.94\n",
    "|RFM|size 2000|1.0|1.0|79.00|78.98\n",
    "|RFM|size 1000|1.0|1.0|79.00|78.94\n",
    "\n",
    "Adjusting trees:\n",
    "| Model | Training Features | Acc Train | F1 Train |Acc Test | F1 Test |\n",
    "|:---|:---|:---|:---|:---|:---|\n",
    "|RFM|n_estimators=150|1.0|1.0|77.75|77.73\n",
    "|RFM|n_estimators=100|1.0|1.0|80.25|80.27\n",
    "|RFM|n_estimators=50|99.87|99.87|81.38|81.39"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
